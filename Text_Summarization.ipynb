{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "WvMFUkx5Xvuq"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------------------------------------------\n",
        "#  Install required libraries for Transformers, Datasets, Evaluation, and Accelerate\n",
        "# ----------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PjOsGVY8xIi",
        "outputId": "18bbfdce-168b-4ec8-9b16-ef906e054a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.41.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.5.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (0.31.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.21.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (23.0.1)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.3.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.78.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.46.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Libraries Installed Successfully!\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers datasets evaluate accelerate tensorflow\n",
        "print(\"Libraries Installed Successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "yf8JK4QBX-Ju"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "#  Import Necessary Libraries\n",
        "# ---------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf9BvQAWKy03",
        "outputId": "43d1a6ed-fe93-4be5-e0c2-e63867e750d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.3.1+cu121\n",
            "4.41.2\n",
            "0.31.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import accelerate\n",
        "\n",
        "print(torch.__version__)\n",
        "print(transformers.__version__)\n",
        "print(accelerate.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "hTOUfkkPYKle"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------\n",
        "#  Load ROUGE metric for evaluation\n",
        "# ---------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "mBUdWNz0LGy2"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "tsIoQ9ShYSp4"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------\n",
        "#  Load a curated summarization dataset from Hugging Face\n",
        "# -------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H9Dsbiv9cuR",
        "outputId": "b528b414-60bb-4cbc-875f-2ee1af36ce9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Text', 'Summary'],\n",
            "        num_rows: 71\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['Text', 'Summary'],\n",
            "        num_rows: 20\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['Text', 'Summary'],\n",
            "        num_rows: 8\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"sudhanshusinghaiml/curated-dataset-for-summarization\")\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "UjcAvo1qYgyx"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------------------\n",
        "#  Explore sample text and summary from the training dataset\n",
        "# ----------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "-_Mz-ECL9u5E",
        "outputId": "c35726a5-effe-44c3-d7f0-94a668ad7731"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Electrical supply company Crescent Electric (CESCO) study reveals that the state of Louisiana is the cheapest state in the US to mine Bitcoin.\\n\\nDigital currency mining requires a lot of electric power and the power rates differ in every state.\\n\\nBased on CESCOâ€™s latest study of the cost of cryptocurrency mining across the US, it is currently cheapest to mine Bitcoin in Louisiana -- electricity costs at 9.87 cents per watt puts the average cost of mining one Bitcoin at $3,224.\\n\\nThis is significantly cheaper than the current price of Bitcoin, which is currently trading at around $12,000 per coin, as of press time.\\n\\nWhere else in the US is it cheap to mine?\\n\\nIn their study, CESCO also estimated the cost of Bitcoin mining based on the wattage consumption of the three most popular mining rigs, namely, the AntMiner S9, the AntMiner S7, and the Avalon 6, as well as the average days each rig takes to mine a token. These figures were then multiplied by the average electricity rate in each state.\\n\\nAside from Louisiana, the other top five states with the lowest cost to mine Bitcoin are Idaho ($3,289 per token), Washington ($3,309), Tennessee ($3,443) and Arkansas ($3,505).\\n\\nThe study also names the most expensive states for digital currency mining. The list of costliest states is led by Hawaii, which takes an average mining cost of $9,483 per coin.\\n\\nRounding up the top five states with the highest Bitcoin mining rates are Alaska ($7,059), Connecticut ($6,951), Massachusetts ($6,674) and New Hampshire ($6,425).\\n\\nThe growing interest in cryptocurrency has been accompanied by growing concern over the energy required to mine crypto, namely Bitcoin. Such claims have been recently countered, as a report came out claiming that put cryptocurrency mining in the larger context of energy consumption.'"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"][\"Text\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "h8vIgVpI_Jt5",
        "outputId": "cf0889cf-291c-4c0e-d3d7-f7e71be4942e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A new study has named Louisiana as the cheapest state in the US in which to mine bitcoin. Electrical supply company Crescent Electric based its calculation on the cost of electricity in each state, the power requirements of the equipment needed, and the average length of time taken to mine a token. This produced a figure of $3,224 per bitcoin for Louisiana, with the most expensive places being Hawaii, at $9,483, and Alaska at $7,059. All of these figures are notably less than the current trading price of bitcoin.\\n'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"][\"Summary\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "MOTr4RbXYrpo"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------------\n",
        "#   Define Model & Preprocessing function to tokenize input text and target summaries\n",
        "# -----------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "0M0w0QjhAI2s"
      },
      "outputs": [],
      "source": [
        "model_name = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "max_input_length = 512\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(dataset):\n",
        "  inputs = dataset[\"Text\"]\n",
        "  target = dataset[\"Summary\"]\n",
        "\n",
        "  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
        "  labels = tokenizer(target, max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "  return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "ezBcbE3tYxPL"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------\n",
        "#  Apply preprocessing function to the entire dataset\n",
        "# ---------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vNiIU6GEvl8",
        "outputId": "c0a09305-9342-48bd-e200-a831f4fe3c15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Text', 'Summary', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 71\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['Text', 'Summary', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 20\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['Text', 'Summary', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 8\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "preprocessed_dataset = dataset.map(preprocess_function, batched=True)\n",
        "print(preprocessed_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEmMja4hE6la",
        "outputId": "150f1076-620f-4ca2-ab94-cedeb2c28456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[86, 17421, 32, 63, 9, 6, 3411, 6, 3, 9, 690, 24, 728, 1213, 46, 1297, 7071, 2425, 12, 20407, 7, 6, 3, 9, 2833, 56, 1116, 617, 7567, 7, 1597, 57, 11509, 12, 165, 1035, 871, 5, 465, 6, 79, 751, 31, 17, 36, 3, 7, 75, 27315, 16, 21, 3730, 10, 86, 2083, 6, 8, 17421, 32, 63, 9, 636, 4457, 56, 17274, 662, 14761, 7, 12, 20395, 4404, 11, 794, 5977, 344, 8242, 21, 3, 9, 215, 5, 17421, 32, 63, 9, 2833, 12, 169, 7567, 7, 21, 23113, 13, 4845, 6, 1397, 10, 634, 282, 9, 107, 23, 4804, 51, 9617, 10, 634, 282, 9, 107, 23, 4804, 51, 9617, 4893, 1303, 17, 5, 509, 87, 115, 107, 40, 157, 172, 329, 632, 448, 29, 102, 3, 318, 282, 9, 107, 23, 4804, 51, 9617, 3, 22356, 518, 41, 1741, 22356, 518, 9, 7, 9, 107, 23, 61, 1762, 1914, 846, 37, 7567, 7, 33, 3, 8317, 1156, 11042, 7, 28, 3, 9, 2777, 18, 9842, 2614, 24, 3, 4610, 30, 16092, 11, 7724, 12, 15305, 190, 8, 2833, 5, 5066, 79, 661, 139, 6917, 6, 79, 31, 60, 2486, 26, 12, 26841, 135, 42, 1977, 6311, 120, 2249, 3, 31, 5420, 75, 1074, 140, 6, 754, 752, 140, 1903, 6, 31, 1315, 12, 37, 282, 9, 107, 23, 4804, 51, 9617, 5, 10071, 54, 23655, 8, 7567, 7, 11, 12317, 3, 9, 3954, 21, 70, 1035, 726, 7134, 338, 3, 9, 7022, 5, 17421, 32, 63, 9, 1192, 8, 7567, 358, 16, 4696, 28, 11509, 18080, 6, 3, 9, 20438, 13, 8, 1510, 8337, 24, 9560, 1510, 1467, 11, 12800, 5, 37, 3689, 661, 56, 661, 8, 7567, 7, 344, 305, 2028, 11, 505, 265, 383, 8, 706, 4108, 116, 3, 10643, 151, 33, 3214, 8, 8242, 5, 5066, 8, 3689, 281, 168, 6, 8, 3064, 164, 854, 12, 17274, 72, 3173, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "print(preprocessed_dataset[\"train\"][\"input_ids\"][10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "_LmNnDjmY0DP"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "#  Remove the original 'Text' column as it is no longer needed\n",
        "# ------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01n-e8c7GZU5",
        "outputId": "c8f0defb-034f-4ac2-d4a8-2004bace4569"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['Summary', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 71\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['Summary', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 20\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['Summary', 'input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 8\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset = preprocessed_dataset.remove_columns(\"Text\")\n",
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "TjqGuEgEY1ff"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "#  Load pre-trained T5 model for sequence-to-sequence learning\n",
        "# ------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "a83cVtZYMBi5"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "2dnL_ejNY2QL"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------\n",
        "#  Define training arguments for Seq2SeqTrainer\n",
        "# ---------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY2nhawMGpA-",
        "outputId": "cd48adfd-53b9-4b00-ddd9-7383aca4634f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./best_model\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"rougeL\",\n",
        "    greater_is_better=True,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=12,\n",
        "    logging_steps=10,\n",
        "    predict_with_generate=True,\n",
        "    report_to=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_2mc_DlQlDg",
        "outputId": "44eceb1d-504c-4ffc-e159-6e668e202dd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "z040hPtQY3W4"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "#   Define function to compute ROUGE metrics during evaluation\n",
        "# ------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "YwaLSKiJH3RJ"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # replace -100 in labels as tokenizer.pad_token_id\n",
        "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    # return only ROUGE-L\n",
        "    return {\"rougeL\": result[\"rougeL\"]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "26A2_7WSY4kX"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------\n",
        "#   Initialize Seq2SeqTrainer with model, tokenizer, datasets, and metrics\n",
        "# ------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "a595445b"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_dataset[\"train\"],\n",
        "    eval_dataset = tokenized_dataset[\"test\"],\n",
        "    tokenizer = tokenizer,\n",
        "    compute_metrics = compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "Wh89rc3dY39q"
      },
      "outputs": [],
      "source": [
        "# ---------------\n",
        "# Train the model\n",
        "# ---------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "fbae1af3",
        "outputId": "403bc098-5fb7-4d87-8718-0cbd01045cdd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [216/216 01:22, Epoch 12/12]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rougel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.563400</td>\n",
              "      <td>2.733088</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.433500</td>\n",
              "      <td>2.716584</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.179400</td>\n",
              "      <td>2.704226</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.151600</td>\n",
              "      <td>2.702156</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=216, training_loss=2.3802934046144837, metrics={'train_runtime': 82.6977, 'train_samples_per_second': 10.303, 'train_steps_per_second': 2.612, 'total_flos': 115311214854144.0, 'train_loss': 2.3802934046144837, 'epoch': 12.0})"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "SS0gnr4qY5MM"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------\n",
        "#  Test the model on a sample from the validation set\n",
        "# ---------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDyhhl-FO9iT",
        "outputId": "5b64c149-05e2-4bec-bebe-9acced2c99f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Summary:\n",
            "JPMorgan Chase has created an internal tool to make sure that its ads don't end up next to unsavory content on YouTube. The company's proprietary algorithm plugs into YouTube's application programming interface (API) to select \"safe\" channels for its ads to advertise on. From more than 5 million channels the brand has wonowed the list down to 3,000 YouTube channels that its ads appear on.\n",
            "\n",
            "Reference Summary:\n",
            "US bank JPMorgan Chase has developed a solution that preventsÂ its ads being placed near unsuitable content on YouTube. A proprietary algorithm with 17 layers of filters that plugs into YouTube's programming systemÂ enablesÂ JPMorgan to whitelist or pre-approve the channels on which its ads are placed. Launched in October amid dissatisfaction with YouTube's own filters, the in-house software has reduced the number of pre-approved sites from five million to 3,000, with a 99.9% success rate. Aaron Smolick, executive director of paid-media analytics and optimisation, said the problem facing companies wasn't brand safety, but brand appropriateness.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sample = tokenized_dataset[\"validation\"][0]\n",
        "\n",
        "input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(model.device)\n",
        "generated_ids = model.generate(input_ids, max_new_tokens=128, num_beams=4)\n",
        "\n",
        "print(\"Generated Summary:\")\n",
        "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
        "\n",
        "print(\"\\nReference Summary:\")\n",
        "print(sample[\"Summary\"])  # remove the [0] here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "VesFXZbRY6HV"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------------------------------------------------------\n",
        "# Save the best trained model and tokenizerxplore sample text and summary from the training dataset.\n",
        "# --------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZceGswySzz7",
        "outputId": "b59d3ba6-b277-43ad-9945-2c897e6742f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 512)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-5): 5 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-5): 5 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.save_model(\"./best_model/final\")\n",
        "tokenizer.save_pretrained(\"./best_model/final\")\n",
        "model.eval()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "rl6T18MdY7DU"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------\n",
        "# Define a reusable function to summarize custom text\n",
        "# ---------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "pHJZzMTQUhlp"
      },
      "outputs": [],
      "source": [
        "def summarize_text(text, max_input_length=512, max_output_length=150, num_beams=5):\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_input_length\n",
        "    ).to(device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=max_output_length,\n",
        "        num_beams=num_beams,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "    return summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "y_Sv8oj6Y70H"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Custom text for inference\n",
        "# -------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-8cHu3ZVkn2",
        "outputId": "accd8ac0-6f2b-414d-ca85-b4a5a92ec1fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Summary:\n",
            " Global shift toward renewable energy is accelerating at an unprecedented pace. Countries around the world are investing in solar, wind, and hydroelectric power to reduce dependence on fossil fuels and combat climate change. Renewable energy sources such as solar and wind are intermittent, producing electricity only when the sun shines or the wind blows.\n"
          ]
        }
      ],
      "source": [
        "custom_text = \"\"\"The global shift toward renewable energy is accelerating at an unprecedented pace. Countries around the world are increasingly investing in solar, wind, and hydroelectric power to reduce dependence on fossil fuels and combat climate change. Solar energy has become particularly popular due to its declining costs and the ability to install panels on both residential and commercial properties. Wind farms, both onshore and offshore, are also expanding rapidly, providing a significant portion of electricity in countries like Germany, the United States, and China.\n",
        "Despite these advances, several challenges remain. One of the biggest obstacles is energy storage. Renewable energy sources such as solar and wind are intermittent, producing electricity only when the sun shines or the wind blows. To address this, companies and governments are investing in battery technology and other storage solutions to ensure a steady and reliable supply of power. Additionally, updating and expanding the electrical grid to handle the influx of renewable energy is critical. Aging infrastructure in many countries poses a barrier to efficient energy distribution and requires substantial investment.\n",
        "Public awareness and government policies also play a crucial role in driving the adoption of renewable energy. Incentives such as tax credits, subsidies, and feed-in tariffs encourage both individuals and businesses to switch to cleaner energy sources. International agreements like the Paris Climate Accord further emphasize the global commitment to reducing greenhouse gas emissions.\n",
        "Technological innovation continues to improve the efficiency and affordability of renewable energy systems. Advances in photovoltaic cells, wind turbine design, and smart grid technology are making renewable energy more competitive with traditional fossil fuels. Experts predict that as renewable energy becomes more widespread, it will reshape energy markets, create new job opportunities, and reduce the environmental impact of power generation.\n",
        "Overall, the transition to renewable energy represents a crucial step toward a sustainable future. While challenges exist, the combined efforts of governments, businesses, and individuals are driving meaningful progress. As innovation continues and investments grow, renewable energy is poised to play a central role in meeting the worldâ€™s energy needs in the coming decades.\"\"\"\n",
        "\n",
        "generated_summary = summarize_text(custom_text)\n",
        "print(\"Generated Summary:\\n\", generated_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "5VObuHTBY8Vi"
      },
      "outputs": [],
      "source": [
        "# ------------------------------\n",
        "# Compare with reference summary\n",
        "# ------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ywX7677VqSu",
        "outputId": "bb4f9289-ba72-428f-8b1d-aa59ef4b6322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reference Summary: The global shift toward renewable energy is accelerating at an unprecedented pace. Countries around the world are increasingly investing in solar, wind, and hydroelectric power to reduce dependence on fossil fuels and combat climate change. Solar energy has become particularly popular due to its declining costs and the ability to install panels on both residential and commercial properties. Wind farms, both onshore and offshore, are also expanding rapidly, providing a significant portion of electricity in countries like Germany, the United States, and China.\n",
            "Despite these advances, several challenges remain. One of the biggest obstacles is energy storage. Renewable energy sources such as solar and wind are intermittent, producing electricity only when the sun shines or the wind blows. To address this, companies and governments are investing in battery technology and other storage solutions to ensure a steady and reliable supply of power. Additionally, updating and expanding the electrical grid to handle the influx of renewable energy is critical. Aging infrastructure in many countries poses a barrier to efficient energy distribution and requires substantial investment.\n",
            "Public awareness and government policies also play a crucial role in driving the adoption of renewable energy. Incentives such as tax credits, subsidies, and feed-in tariffs encourage both individuals and businesses to switch to cleaner energy sources. International agreements like the Paris Climate Accord further emphasize the global commitment to reducing greenhouse gas emissions.\n",
            "Technological innovation continues to improve the efficiency and affordability of renewable energy systems. Advances in photovoltaic cells, wind turbine design, and smart grid technology are making renewable energy more competitive with traditional fossil fuels. Experts predict that as renewable energy becomes more widespread, it will reshape energy markets, create new job opportunities, and reduce the environmental impact of power generation.\n",
            "Overall, the transition to renewable energy represents a crucial step toward a sustainable future. While challenges exist, the combined efforts of governments, businesses, and individuals are driving meaningful progress. As innovation continues and investments grow, renewable energy is poised to play a central role in meeting the worldâ€™s energy needs in the coming decades\n"
          ]
        }
      ],
      "source": [
        "Reference_Summary = \"\"\"The global shift toward renewable energy is accelerating at an unprecedented pace. Countries around the world are increasingly investing in solar, wind, and hydroelectric power to reduce dependence on fossil fuels and combat climate change. Solar energy has become particularly popular due to its declining costs and the ability to install panels on both residential and commercial properties. Wind farms, both onshore and offshore, are also expanding rapidly, providing a significant portion of electricity in countries like Germany, the United States, and China.\n",
        "Despite these advances, several challenges remain. One of the biggest obstacles is energy storage. Renewable energy sources such as solar and wind are intermittent, producing electricity only when the sun shines or the wind blows. To address this, companies and governments are investing in battery technology and other storage solutions to ensure a steady and reliable supply of power. Additionally, updating and expanding the electrical grid to handle the influx of renewable energy is critical. Aging infrastructure in many countries poses a barrier to efficient energy distribution and requires substantial investment.\n",
        "Public awareness and government policies also play a crucial role in driving the adoption of renewable energy. Incentives such as tax credits, subsidies, and feed-in tariffs encourage both individuals and businesses to switch to cleaner energy sources. International agreements like the Paris Climate Accord further emphasize the global commitment to reducing greenhouse gas emissions.\n",
        "Technological innovation continues to improve the efficiency and affordability of renewable energy systems. Advances in photovoltaic cells, wind turbine design, and smart grid technology are making renewable energy more competitive with traditional fossil fuels. Experts predict that as renewable energy becomes more widespread, it will reshape energy markets, create new job opportunities, and reduce the environmental impact of power generation.\n",
        "Overall, the transition to renewable energy represents a crucial step toward a sustainable future. While challenges exist, the combined efforts of governments, businesses, and individuals are driving meaningful progress. As innovation continues and investments grow, renewable energy is poised to play a central role in meeting the worldâ€™s energy needs in the coming decades\"\"\"\n",
        "\n",
        "print(\"Reference Summary:\",Reference_Summary)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
